import os
import time
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader
from torchvision.utils import save_image  # ç”¨äºä¿å­˜å›¾ç‰‡

from config import Config
from models.network import CWAF_Net
from utils.dataset import FusionDataset
from utils.metrics import FusionMetrics


def test_and_evaluate():
    # 1. åˆå§‹åŒ–é…ç½®
    cfg = Config()
    device = cfg.device
    print(f"ğŸš€ Start Testing & Evaluation on {device}...")

    # ================= æ–‡ä»¶ä¿å­˜è·¯å¾„è®¾ç½® =================
    # ç»“æœæ€»ç›®å½•: results/
    # å›¾ç‰‡ä¿å­˜: results/fused_images/
    # æŠ¥å‘Šä¿å­˜: results/evaluation_report.txt
    output_root = "results"
    image_save_dir = os.path.join(output_root, "fused_images")
    report_path = os.path.join(output_root, "evaluation_report.txt")

    os.makedirs(image_save_dir, exist_ok=True)
    # ===================================================

    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    # ä¼˜å…ˆæ‰¾ 'test' ç›®å½•ï¼Œæ²¡æœ‰åˆ™ç”¨ 'train' ä»£æ›¿æ¼”ç¤º
    test_dataset = FusionDataset(cfg.DATA_ROOT, mode='test', img_size=cfg.img_size)
    if len(test_dataset) == 0:
        print("âš ï¸ Warning: Test dataset empty, using Train dataset for demo.")
        test_dataset = FusionDataset(cfg.DATA_ROOT, mode='train', img_size=cfg.img_size)

    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

    # 3. åŠ è½½æ¨¡å‹
    model = CWAF_Net(in_channels=cfg.in_channels, feat_dim=cfg.feat_dim).to(device)

    # ğŸ”´ è¯·ç¡®è®¤è¿™æ˜¯æ‚¨æƒ³æµ‹è¯•çš„æƒé‡æ–‡ä»¶
    checkpoint_path = os.path.join(cfg.CHECKPOINT_DIR, "cwaf_epoch_100.pth")

    if os.path.exists(checkpoint_path):
        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        print(f"âœ… Model loaded from {checkpoint_path}")
    else:
        print(f"âŒ Error: Checkpoint not found at {checkpoint_path}!")
        return

    model.eval()

    # 4. åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
    metrics_calc = FusionMetrics(device=device)

    # æŒ‡æ ‡ç»“æœå­˜å‚¨åˆ—è¡¨
    metric_results = {
        "EN": [], "SD": [], "SF": [], "AG": [],
        "MI": [], "SCD": [], "Qabf": [], "SSIM": []
    }

    print(f"Processing {len(test_dataset)} image pairs...")

    # å¼€å§‹æ¨ç†
    with torch.no_grad():
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        for i, (img_a, img_b, names) in enumerate(tqdm(test_loader, ncols=100)):
            img_a = img_a.to(device)
            img_b = img_b.to(device)

            # --- A. æ¨ç† (Inference) ---
            fused = model(img_a, img_b)
            fused = torch.clamp(fused, 0, 1)  # ç¡®ä¿åƒç´ å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…

            # --- B. ä¿å­˜å›¾ç‰‡ (Save Images) ---
            # å‡è®¾ dataset è¿”å›çš„æ–‡ä»¶ååœ¨ names å…ƒç»„é‡Œï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨ç´¢å¼•å‘½å
            file_name = f"{i + 1:03d}_fused.png"
            save_path = os.path.join(image_save_dir, file_name)
            save_image(fused, save_path)

            # --- C. è®¡ç®—æŒ‡æ ‡ (Calculate Metrics) ---
            metric_results["EN"].append(metrics_calc.EN(fused))
            metric_results["SD"].append(metrics_calc.SD(fused))
            metric_results["SF"].append(metrics_calc.SF(fused))
            metric_results["AG"].append(metrics_calc.AG(fused))
            metric_results["MI"].append(metrics_calc.MI(fused, img_a, img_b))
            metric_results["SCD"].append(metrics_calc.SCD(fused, img_a, img_b))
            metric_results["Qabf"].append(metrics_calc.Qabf(fused, img_a, img_b))
            metric_results["SSIM"].append(metrics_calc.MS_SSIM(fused, img_a, img_b))

    # 5. æ•´ç†ç»“æœå¹¶ä¿å­˜
    current_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

    # å‡†å¤‡æŠ¥å‘Šå†…å®¹çš„å­—ç¬¦ä¸²
    report_lines = []
    report_lines.append(f"==================================================")
    report_lines.append(f"ğŸ“… Evaluation Report - {current_time}")
    report_lines.append(f"ğŸ¤– Model: {checkpoint_path}")
    report_lines.append(f"ğŸ–¼ï¸  Test Set Size: {len(test_dataset)}")
    report_lines.append(f"==================================================")
    report_lines.append(f"{'Metric':<10} | {'Average':<10} | {'Std Dev':<10}")
    report_lines.append(f"--------------------------------------------------")

    # æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + "\n".join(report_lines))

    # éå†æŒ‡æ ‡è®¡ç®—å¹³å‡å€¼
    for key, val_list in metric_results.items():
        avg_val = np.mean(val_list)
        std_val = np.std(val_list)
        line = f"{key:<10} | {avg_val:<10.4f} | {std_val:<10.4f}"
        print(line)
        report_lines.append(line)

    print("==================================================\n")

    # --- D. å†™å…¥ TXT æ–‡ä»¶ (Appendæ¨¡å¼ï¼Œä¸è¦†ç›–æ—§è®°å½•) ---
    with open(report_path, "a", encoding="utf-8") as f:
        f.write("\n".join(report_lines))
        f.write("\n\n")  # ç©ºä¸¤è¡Œï¼Œæ–¹ä¾¿åŒºåˆ†ä¸‹ä¸€æ¬¡è®°å½•

    print(f"ğŸ‰ All Done!")
    print(f"   - Images saved to: {image_save_dir}")
    print(f"   - Report saved to: {report_path}")


if __name__ == "__main__":
    test_and_evaluate()