import os
import time
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader
from torchvision.utils import save_image  # ç”¨äºä¿å­˜å›¾ç‰‡

from config import Config
from models.network import CWAF_Net
from utils.dataset import FusionDataset
from utils.metrics import FusionMetrics


def test_and_evaluate():
    # 1. åˆå§‹åŒ–é…ç½®
    cfg = Config()
    device = cfg.device
    print(f"ğŸš€ Start Testing & Evaluation on {device}...")

    # ================= æ–‡ä»¶ä¿å­˜è·¯å¾„è®¾ç½® =================
    output_root = "results"
    image_save_dir = os.path.join(output_root, "fused_images")
    report_path = os.path.join(output_root, "evaluation_report.txt")

    os.makedirs(image_save_dir, exist_ok=True)
    # ===================================================

    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    test_dataset = FusionDataset(cfg.DATA_ROOT, mode='test', img_size=cfg.img_size)
    if len(test_dataset) == 0:
        print("âš ï¸ Warning: Test dataset empty, using Train dataset for demo.")
        test_dataset = FusionDataset(cfg.DATA_ROOT, mode='train', img_size=cfg.img_size)

    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

    # 3. åŠ è½½æ¨¡å‹
    model = CWAF_Net(in_channels=cfg.in_channels, feat_dim=cfg.feat_dim).to(device)

    # è¯·ç¡®è®¤è¿™æ˜¯æ‚¨æƒ³æµ‹è¯•çš„æƒé‡æ–‡ä»¶
    checkpoint_path = os.path.join(cfg.CHECKPOINT_DIR, "cwaf_epoch_40.pth")

    if os.path.exists(checkpoint_path):
        # âš ï¸ æ³¨æ„: å¦‚æœä¹‹å‰æ”¹äº† InstanceNormï¼Œè®°å¾—è¿™é‡Œè¦ä¸è¦åŠ  strict=False å–å†³äºæ‚¨çš„æƒé‡æ˜¯æ–°è®­ç»ƒçš„è¿˜æ˜¯æ—§çš„
        # å¦‚æœæ˜¯æ–°è®­ç»ƒçš„ï¼Œä¸éœ€è¦ strict=False
        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        print(f"âœ… Model loaded from {checkpoint_path}")
    else:
        print(f"âŒ Error: Checkpoint not found at {checkpoint_path}!")
        return

    model.eval()

    # 4. åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
    metrics_calc = FusionMetrics(device=device)

    # æŒ‡æ ‡ç»“æœå­˜å‚¨åˆ—è¡¨ (å·²åˆ é™¤ AG, åŠ å…¥ VIF)
    metric_results = {
        "EN": [], "SD": [], "SF": [], "VIF": [],  # <--- VIF æ›¿æ¢äº† AG
        "MI": [], "SCD": [], "Qabf": [], "SSIM": []
    }

    print(f"Processing {len(test_dataset)} image pairs...")

    # å¼€å§‹æ¨ç†
    with torch.no_grad():
        for i, (img_a, img_b, names) in enumerate(tqdm(test_loader, ncols=100)):
            img_a = img_a.to(device)
            img_b = img_b.to(device)

            # --- A. æ¨ç† (Inference) ---
            fused = model(img_a, img_b)
            fused = torch.clamp(fused, 0, 1)  # ç¡®ä¿åƒç´ å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…

            # --- B. ä¿å­˜å›¾ç‰‡ (Save Images) ---
            file_name = f"{i + 1:03d}_fused.png"
            save_path = os.path.join(image_save_dir, file_name)
            save_image(fused, save_path)

            # --- C. è®¡ç®—æŒ‡æ ‡ (Calculate Metrics) ---
            metric_results["EN"].append(metrics_calc.EN(fused))
            metric_results["SD"].append(metrics_calc.SD(fused))
            metric_results["SF"].append(metrics_calc.SF(fused))

            # ã€ä¿®æ”¹ç‚¹ã€‘è°ƒç”¨ VIF è€Œä¸æ˜¯ AG
            metric_results["VIF"].append(metrics_calc.VIF(fused, img_a, img_b))

            metric_results["MI"].append(metrics_calc.MI(fused, img_a, img_b))
            metric_results["SCD"].append(metrics_calc.SCD(fused, img_a, img_b))
            metric_results["Qabf"].append(metrics_calc.Qabf(fused, img_a, img_b))
            metric_results["SSIM"].append(metrics_calc.Avg_SSIM(fused, img_a, img_b))

    # 5. æ•´ç†ç»“æœå¹¶ä¿å­˜
    current_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

    report_lines = []
    report_lines.append(f"==================================================")
    report_lines.append(f"ğŸ“… Evaluation Report - {current_time}")
    report_lines.append(f"ğŸ¤– Model: {checkpoint_path}")
    report_lines.append(f"ğŸ–¼ï¸  Test Set Size: {len(test_dataset)}")
    report_lines.append(f"==================================================")
    report_lines.append(f"{'Metric':<10} | {'Average':<10} | {'Std Dev':<10}")
    report_lines.append(f"--------------------------------------------------")

    print("\n" + "\n".join(report_lines))

    for key, val_list in metric_results.items():
        avg_val = np.mean(val_list)
        std_val = np.std(val_list)
        line = f"{key:<10} | {avg_val:<10.4f} | {std_val:<10.4f}"
        print(line)
        report_lines.append(line)

    print("==================================================\n")

    with open(report_path, "a", encoding="utf-8") as f:
        f.write("\n".join(report_lines))
        f.write("\n\n")

    print(f"ğŸ‰ All Done!")
    print(f"   - Images saved to: {image_save_dir}")
    print(f"   - Report saved to: {report_path}")


if __name__ == "__main__":
    test_and_evaluate()